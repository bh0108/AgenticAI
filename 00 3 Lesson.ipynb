{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b15f8d8",
   "metadata": {},
   "source": [
    "# This notebook demonstrates how to implement and train a simple linear regression model using PyTorch.\n",
    "# It covers data generation, model definition, loss function and optimizer setup, training loop, and evaluation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a392e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Prepare Data\n",
    "# Generate some synthetic data for linear regression: y = 2*x + 1 + noise\n",
    "X = torch.randn(100, 1) * 10  # 100 data points, 1 feature\n",
    "y = 2 * X + 1 + torch.randn(100, 1) * 2  # True relationship + noise\n",
    "\n",
    "# 2. Define the Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1) # One input feature, one output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# 3. Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # Stochastic Gradient Descent with learning rate\n",
    "\n",
    "# 4. Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Backward and optimize: Zero gradients, perform backpropagation, update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 5. Make Predictions (Optional)\n",
    "predicted = model(X).detach().numpy() # Detach from computation graph and convert to NumPy\n",
    "print(\"\\nFirst 5 true values:\", y[:5].numpy().flatten())\n",
    "print(\"First 5 predicted values:\", predicted[:5].flatten())\n",
    "\n",
    "# Print learned parameters\n",
    "print(\"\\nLearned Weight:\", model.linear.weight.item())\n",
    "print(\"Learned Bias:\", model.linear.bias.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721367c",
   "metadata": {},
   "source": [
    "## Problem Summary:\n",
    "We are tasked with predicting daily sales of FMCG products from a warehouse, using both historical sales data and local weather (temperature) information.\n",
    "\n",
    "## Solution Overview:\n",
    "We will use a linear regression model to learn the relationship between temperature and product sales. The model will be trained on historical data, and then used to predict future sales based on temperature forecasts.\n",
    "\n",
    "## Next Steps:\n",
    "### 1. Load and explore the sales and temperature datasets.\n",
    "### 2. Prepare the data for modeling (e.g., merging, cleaning, feature selection).\n",
    "### 3. Train the linear regression model.\n",
    "### 4. Evaluate model performance and make predictions.\n",
    "### 5. Save and reload the model for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc001f",
   "metadata": {},
   "source": [
    "1. Define a use case and dataset associated with it\n",
    "2. Use the Linear Regression Model for the use case\n",
    "3. Ability to save a model as a checkpoint\n",
    "4. Retrieve the model and retrain with new training dataset\n",
    "5. Validation and model performance\n",
    "6. Scoring pipeline using quick API design\n",
    "7. Key learnings and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e8ec4",
   "metadata": {},
   "source": [
    "You are managing a FMCG product warehouse and you need to manage the products sold from the warehouse along with dependant weather patterns around the warehouse location.\n",
    "\n",
    "You are given a data about the sales daily and you are given the temperature of the town where the warehouse is located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sales_data = pd.read_csv('ohio_daily_vegetable_sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = pd.read_csv('ohio_daily_avg_temperature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomatoes_data = sales_data[sales_data.Vegetable=='Tomatoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomatoes_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = tomatoes_data.Revenue_USD.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = temp_data.Avg_Temperature_F.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82e085",
   "metadata": {},
   "source": [
    "X is an independent variable and y is the depenedant variable for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant (intercept term)\n",
    "X_2 = sm.add_constant(X_1)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = sm.OLS(y_1, X_2).fit()\n",
    "\n",
    "# Print summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-09-01\n",
    "t = 29.1\n",
    "actual_sales = 1083.6\n",
    "\n",
    "# linear model \n",
    "pred_sales = -25 * t + 2225\n",
    "\n",
    "print(actual_sales)\n",
    "print(pred_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-12-25\n",
    "t = 15\n",
    "actual_sales = '?'\n",
    "\n",
    "# linear model \n",
    "pred_sales = -25 * t + 2225\n",
    "\n",
    "print(actual_sales)\n",
    "print(pred_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa902968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Prepare Data\n",
    "# ensure float32 instead of double\n",
    "X = torch.tensor(X_1, dtype=torch.float32).view(-1, 1)   # shape [N,1]\n",
    "y = (-25 * X + 2225).view(-1, 1)                         # same shape as output\n",
    "\n",
    "# 2. Define the Model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1) # default float32\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# 3. Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.9)\n",
    "\n",
    "# 4. Training Loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 5. Make Predictions\n",
    "predicted = model(X).detach().numpy()\n",
    "print(\"\\nFirst 5 true values:\", y[:5].numpy().flatten())\n",
    "print(\"First 5 predicted values:\", predicted[:5].flatten())\n",
    "\n",
    "# Print learned parameters\n",
    "print(\"\\nLearned Weight:\", model.linear.weight.item())\n",
    "print(\"Learned Bias:\", model.linear.bias.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584e3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sales = 224.11 * t + 5.05\n",
    "\n",
    "print(pred_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390abd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff068117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = \"<Your-IAM-Role-ARN>\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.12\",\n",
    "    py_version=\"py38\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "estimator.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ad888",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c37ed4b",
   "metadata": {},
   "source": [
    "## What this code does:\n",
    "\n",
    "1. **Creates a checkpoint directory** - Makes a folder called `model_checkpoints` to store your saved models\n",
    "\n",
    "2. **Saves the complete checkpoint** - Saves not just the model weights, but also:\n",
    "   - Model state dictionary (weights and biases)\n",
    "   - Optimizer state (for resuming training)\n",
    "   - Current epoch number\n",
    "   - Final loss value\n",
    "   - Model configuration\n",
    "\n",
    "3. **Loads the checkpoint** - Demonstrates how to:\n",
    "   - Create a new model instance\n",
    "   - Load the saved weights\n",
    "   - Verify the loaded model matches the original\n",
    "\n",
    "4. **Tests the loaded model** - Makes predictions with both models to ensure they're identical\n",
    "\n",
    "## Key benefits of this approach:\n",
    "\n",
    "- **Resume training**: You can continue training from where you left off\n",
    "- **Model sharing**: Save and share your trained model with others\n",
    "- **Production deployment**: Load the trained model for inference\n",
    "- **Experiment tracking**: Keep track of different model versions\n",
    "\n",
    "You can add this code to your notebook after your training loop to save your model checkpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c30ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save Model Checkpoint\n",
    "print(\"\\n=== Saving Model Checkpoint ===\")\n",
    "\n",
    "# Create checkpoint directory if it doesn't exist\n",
    "import os\n",
    "checkpoint_dir = \"model_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save the complete model checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"linear_regression_checkpoint.pth\")\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "    'model_config': {\n",
    "        'in_features': 1,\n",
    "        'out_features': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Model checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "# 7. Load Model Checkpoint\n",
    "print(\"\\n=== Loading Model Checkpoint ===\")\n",
    "\n",
    "# Create a new model instance\n",
    "loaded_model = LinearRegression()\n",
    "\n",
    "# Load the checkpoint\n",
    "loaded_checkpoint = torch.load(checkpoint_path)\n",
    "loaded_model.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Verify the loaded model has the same parameters\n",
    "print(f\"Original model weight: {model.linear.weight.item():.4f}\")\n",
    "print(f\"Loaded model weight: {loaded_model.linear.weight.item():.4f}\")\n",
    "print(f\"Original model bias: {model.linear.bias.item():.4f}\")\n",
    "print(f\"Loaded model bias: {loaded_model.linear.bias.item():.4f}\")\n",
    "\n",
    "# Test prediction with loaded model\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor([[25.0]], dtype=torch.float32)  # Test temperature of 25°F\n",
    "    original_prediction = model(test_input).item()\n",
    "    loaded_prediction = loaded_model(test_input).item()\n",
    "    \n",
    "    print(f\"\\nTest prediction at 25°F:\")\n",
    "    print(f\"Original model: {original_prediction:.2f}\")\n",
    "    print(f\"Loaded model: {loaded_prediction:.2f}\")\n",
    "\n",
    "print(\"\\nModel checkpoint successfully saved and loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95688ab2",
   "metadata": {},
   "source": [
    "I'll show you the code for model validation and performance evaluation. Here's comprehensive code to validate your model and assess its performance:\n",
    "\n",
    "## What this validation code provides:\n",
    "\n",
    "### 1. **Data Splitting**\n",
    "- Splits data into training (80%) and validation (20%) sets\n",
    "- Ensures unbiased performance evaluation\n",
    "\n",
    "### 2. **Training with Validation**\n",
    "- Monitors both training and validation loss during training\n",
    "- Helps detect overfitting early\n",
    "\n",
    "### 3. **Comprehensive Metrics**\n",
    "- **MSE/RMSE**: Measures prediction error\n",
    "- **MAE**: Mean absolute error (robust to outliers)\n",
    "- **R² Score**: Proportion of variance explained (0-1, higher is better)\n",
    "\n",
    "### 4. **Overfitting Detection**\n",
    "- Compares training vs validation performance\n",
    "- Warns about potential overfitting or underfitting\n",
    "\n",
    "### 5. **Residual Analysis**\n",
    "- Checks if predictions are unbiased\n",
    "- Analyzes error distribution\n",
    "\n",
    "### 6. **Cross-Validation**\n",
    "- K-fold cross-validation for robust performance estimation\n",
    "- Provides confidence intervals for performance metrics\n",
    "\n",
    "### 7. **Baseline Comparison**\n",
    "- Compares against simple baseline (mean prediction)\n",
    "- Shows actual improvement over naive approaches\n",
    "\n",
    "### 8. **Model Persistence**\n",
    "- Saves the validated model with performance metrics\n",
    "- Enables reproducible results\n",
    "\n",
    "This comprehensive validation approach will give you confidence in your model's performance and help identify any issues before deployment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Validation and Performance Evaluation\n",
    "print(\"\\n=== Model Validation and Performance ===\")\n",
    "\n",
    "# 1. Split data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Convert PyTorch tensors to numpy for sklearn functions\n",
    "X_np = X.numpy().flatten()\n",
    "y_np = y.numpy().flatten()\n",
    "\n",
    "# Split data (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, y_np, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert back to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).view(-1, 1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 2. Train model on training data only\n",
    "print(\"Training model on training data...\")\n",
    "train_model = LinearRegression()\n",
    "train_criterion = nn.MSELoss()\n",
    "train_optimizer = optim.Adam(train_model.parameters(), lr=0.9)\n",
    "\n",
    "train_epochs = 3000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    # Training\n",
    "    train_model.train()\n",
    "    train_outputs = train_model(X_train_tensor)\n",
    "    train_loss = train_criterion(train_outputs, y_train_tensor)\n",
    "    \n",
    "    train_optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    train_optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    train_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = train_model(X_val_tensor)\n",
    "        val_loss = train_criterion(val_outputs, y_val_tensor)\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{train_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# 3. Model Performance Metrics\n",
    "print(\"\\n=== Performance Metrics ===\")\n",
    "\n",
    "# Make predictions on validation set\n",
    "train_model.eval()\n",
    "with torch.no_grad():\n",
    "    val_predictions = train_model(X_val_tensor).numpy().flatten()\n",
    "    train_predictions = train_model(X_train_tensor).numpy().flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "train_mae = mean_absolute_error(y_train, train_predictions)\n",
    "val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "val_r2 = r2_score(y_val, val_predictions)\n",
    "\n",
    "print(f\"Training Set Metrics:\")\n",
    "print(f\"  MSE: {train_mse:.2f}\")\n",
    "print(f\"  RMSE: {train_rmse:.2f}\")\n",
    "print(f\"  MAE: {train_mae:.2f}\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Set Metrics:\")\n",
    "print(f\"  MSE: {val_mse:.2f}\")\n",
    "print(f\"  RMSE: {val_rmse:.2f}\")\n",
    "print(f\"  MAE: {val_mae:.2f}\")\n",
    "print(f\"  R² Score: {val_r2:.4f}\")\n",
    "\n",
    "# 4. Overfitting Check\n",
    "print(f\"\\n=== Overfitting Analysis ===\")\n",
    "overfitting_ratio = val_mse / train_mse if train_mse > 0 else float('inf')\n",
    "print(f\"Validation/Training MSE Ratio: {overfitting_ratio:.2f}\")\n",
    "\n",
    "if overfitting_ratio > 1.5:\n",
    "    print(\"⚠️  Potential overfitting detected (validation loss much higher than training loss)\")\n",
    "elif overfitting_ratio < 0.8:\n",
    "    print(\"⚠️  Potential underfitting detected (validation loss much lower than training loss)\")\n",
    "else:\n",
    "    print(\"✅ Good generalization (validation and training losses are similar)\")\n",
    "\n",
    "# 5. Residual Analysis\n",
    "print(f\"\\n=== Residual Analysis ===\")\n",
    "residuals = y_val - val_predictions\n",
    "\n",
    "print(f\"Residual Statistics:\")\n",
    "print(f\"  Mean: {np.mean(residuals):.2f}\")\n",
    "print(f\"  Std: {np.std(residuals):.2f}\")\n",
    "print(f\"  Min: {np.min(residuals):.2f}\")\n",
    "print(f\"  Max: {np.max(residuals):.2f}\")\n",
    "\n",
    "# Check for bias in residuals\n",
    "if abs(np.mean(residuals)) > 0.1 * np.std(residuals):\n",
    "    print(\"⚠️  Residuals show potential bias (mean significantly different from 0)\")\n",
    "else:\n",
    "    print(\"✅ Residuals appear unbiased\")\n",
    "\n",
    "# 6. Cross-Validation (K-Fold)\n",
    "print(f\"\\n=== K-Fold Cross-Validation ===\")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "\n",
    "# Use sklearn's LinearRegression for cross-validation\n",
    "sklearn_model = SklearnLinearRegression()\n",
    "cv_scores = cross_val_score(sklearn_model, X_np.reshape(-1, 1), y_np, cv=5, scoring='r2')\n",
    "\n",
    "print(f\"Cross-validation R² scores: {cv_scores}\")\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# 7. Model Comparison with Baseline\n",
    "print(f\"\\n=== Baseline Comparison ===\")\n",
    "# Simple baseline: predict mean of training data\n",
    "baseline_predictions = np.full_like(y_val, np.mean(y_train))\n",
    "baseline_mse = mean_squared_error(y_val, baseline_predictions)\n",
    "baseline_r2 = r2_score(y_val, baseline_predictions)\n",
    "\n",
    "print(f\"Baseline (Mean Prediction):\")\n",
    "print(f\"  MSE: {baseline_mse:.2f}\")\n",
    "print(f\"  R² Score: {baseline_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nModel vs Baseline:\")\n",
    "print(f\"  MSE Improvement: {((baseline_mse - val_mse) / baseline_mse * 100):.1f}%\")\n",
    "print(f\"  R² Improvement: {((val_r2 - baseline_r2) / (1 - baseline_r2) * 100):.1f}%\")\n",
    "\n",
    "# 8. Save the validated model\n",
    "print(f\"\\n=== Saving Validated Model ===\")\n",
    "validated_checkpoint_path = os.path.join(checkpoint_dir, \"validated_linear_regression.pth\")\n",
    "validated_checkpoint = {\n",
    "    'epoch': train_epochs,\n",
    "    'model_state_dict': train_model.state_dict(),\n",
    "    'optimizer_state_dict': train_optimizer.state_dict(),\n",
    "    'train_loss': train_losses[-1],\n",
    "    'val_loss': val_losses[-1],\n",
    "    'performance_metrics': {\n",
    "        'train_mse': train_mse,\n",
    "        'val_mse': val_mse,\n",
    "        'train_r2': train_r2,\n",
    "        'val_r2': val_r2,\n",
    "        'cv_mean_r2': cv_scores.mean(),\n",
    "        'cv_std_r2': cv_scores.std()\n",
    "    },\n",
    "    'model_config': {\n",
    "        'in_features': 1,\n",
    "        'out_features': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(validated_checkpoint, validated_checkpoint_path)\n",
    "print(f\"Validated model saved to: {validated_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4add01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the same model architecture as used in training\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, in_features=1, out_features=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Load the trained model\n",
    "MODEL_PATH = \"checkpoints/validated_linear_regression.pth\"\n",
    "model = LinearRegressionModel()\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device('cpu'), weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    x: float\n",
    "\n",
    "class OutputData(BaseModel):\n",
    "    prediction: float\n",
    "\n",
    "@app.post(\"/predict\", response_model=OutputData)\n",
    "def predict(data: InputData):\n",
    "    # Prepare input for PyTorch model\n",
    "    x_tensor = torch.tensor([[data.x]], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_tensor)\n",
    "    return OutputData(prediction=float(y_pred.item()))\n",
    "\n",
    "# To run: uvicorn <filename>:app --reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee61ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Sample code to test the FastAPI endpoint\n",
    "def test_fastapi_predict():\n",
    "    url = \"http://127.0.0.1:8000/predict\"\n",
    "    payload = {\"x\": 2.5}\n",
    "    response = requests.post(url, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Prediction:\", response.json())\n",
    "    else:\n",
    "        print(\"Request failed with status code:\", response.status_code)\n",
    "        print(\"Response:\", response.text)\n",
    "\n",
    "# Uncomment the following line to run the test\n",
    "# test_fastapi_predict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
