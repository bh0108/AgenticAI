{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Basic Concepts of LLM Models using Hugging Face\n",
        "\n",
        "## Course Overview\n",
        "This notebook introduces Large Language Models (LLMs) and how to work with them using the Hugging Face library. LLMs are the \"brain\" of agentic AI systems, providing reasoning and language understanding capabilities.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand what Large Language Models (LLMs) are\n",
        "- Learn about the Hugging Face ecosystem\n",
        "- Explore different types of LLM architectures\n",
        "- Learn to load and use pre-trained models\n",
        "- Understand tokenization and text generation\n",
        "- Explore model parameters and inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What are Large Language Models (LLMs)?\n",
        "\n",
        "**Large Language Models** are AI systems trained on vast amounts of text data to:\n",
        "- Understand and generate human-like text\n",
        "- Answer questions\n",
        "- Perform language tasks (translation, summarization, etc.)\n",
        "- Reason about complex problems\n",
        "\n",
        "Key characteristics:\n",
        "- **Transformer architecture**: The underlying neural network design\n",
        "- **Pre-training**: Models learn from massive text corpora\n",
        "- **Fine-tuning**: Models can be adapted for specific tasks\n",
        "- **Context window**: The amount of text a model can process at once\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Hugging Face\n",
        "\n",
        "**Hugging Face** is a platform and library that provides:\n",
        "- Pre-trained models (Transformers library)\n",
        "- Tokenizers for text processing\n",
        "- Model Hub for sharing models\n",
        "- Easy-to-use APIs for working with LLMs\n",
        "\n",
        "Let's start by installing and importing the necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install transformers torch accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Hugging Face Transformers library loaded successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Tokenization\n",
        "\n",
        "**Tokenization** is the process of converting text into tokens (smaller units) that the model can understand. Different models use different tokenizers.\n",
        "\n",
        "Let's explore tokenization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a tokenizer (using GPT-2 as an example - lightweight model)\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example text\n",
        "text = \"Hello! How are you today? I'm learning about LLMs.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.encode(text)\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Tokenized (words/subwords):\")\n",
        "print(tokens)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Token IDs (numerical representation):\")\n",
        "print(token_ids)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "print(f\"Number of characters: {len(text)}\")\n",
        "\n",
        "# Decode back to text\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(\"\\nDecoded back to text:\")\n",
        "print(decoded_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Pre-trained Models\n",
        "\n",
        "Hugging Face makes it easy to load pre-trained models. Let's load a small model for demonstration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a small, efficient model for demonstration\n",
        "# Note: For production, you might use larger models like Llama, Mistral, etc.\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "print(\"This may take a moment on first run (downloading model)...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set pad token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"\\nModel loaded successfully!\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Generation with LLMs\n",
        "\n",
        "Now let's generate text using the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_length=50, temperature=0.7, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text using the loaded model\n",
        "    \n",
        "    Parameters:\n",
        "    - prompt: Input text to continue\n",
        "    - max_length: Maximum length of generated text\n",
        "    - temperature: Controls randomness (lower = more deterministic)\n",
        "    - top_k: Limits sampling to top k tokens\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example 1: Simple generation\n",
        "prompt1 = \"The future of artificial intelligence\"\n",
        "result1 = generate_text(prompt1, max_length=80)\n",
        "print(\"Prompt:\", prompt1)\n",
        "print(\"Generated:\", result1)\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Example 2: Different temperature\n",
        "prompt2 = \"Once upon a time\"\n",
        "result2_low_temp = generate_text(prompt2, max_length=60, temperature=0.3)\n",
        "result2_high_temp = generate_text(prompt2, max_length=60, temperature=1.2)\n",
        "\n",
        "print(\"Prompt:\", prompt2)\n",
        "print(\"\\nLow temperature (0.3) - more deterministic:\")\n",
        "print(result2_low_temp)\n",
        "print(\"\\nHigh temperature (1.2) - more creative:\")\n",
        "print(result2_high_temp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Hugging Face Pipelines\n",
        "\n",
        "Hugging Face provides convenient **pipelines** that simplify common tasks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "# Generate text\n",
        "prompt = \"Artificial intelligence will\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(result[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Model Parameters\n",
        "\n",
        "Key parameters that control LLM behavior:\n",
        "\n",
        "1. **Temperature**: Controls randomness (0.0 = deterministic, 1.0+ = creative)\n",
        "2. **Top-k**: Samples from top k most likely tokens\n",
        "3. **Top-p (nucleus sampling)**: Samples from tokens with cumulative probability p\n",
        "4. **Max length**: Maximum number of tokens to generate\n",
        "5. **Repetition penalty**: Reduces repetition in generated text\n",
        "\n",
        "Let's see how these affect generation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_generation_parameters(prompt):\n",
        "    \"\"\"Compare different generation parameters\"\"\"\n",
        "    print(f\"Prompt: '{prompt}'\\n\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Low temperature\n",
        "    result1 = generate_text(prompt, max_length=40, temperature=0.1)\n",
        "    print(\"Low Temperature (0.1) - Very deterministic:\")\n",
        "    print(result1)\n",
        "    print()\n",
        "    \n",
        "    # Medium temperature\n",
        "    result2 = generate_text(prompt, max_length=40, temperature=0.7)\n",
        "    print(\"Medium Temperature (0.7) - Balanced:\")\n",
        "    print(result2)\n",
        "    print()\n",
        "    \n",
        "    # High temperature\n",
        "    result3 = generate_text(prompt, max_length=40, temperature=1.5)\n",
        "    print(\"High Temperature (1.5) - Very creative:\")\n",
        "    print(result3)\n",
        "    print()\n",
        "\n",
        "# Test with a prompt\n",
        "compare_generation_parameters(\"The secret to success is\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different LLM Architectures\n",
        "\n",
        "Common LLM architectures available on Hugging Face:\n",
        "\n",
        "1. **GPT (Generative Pre-trained Transformer)**: Autoregressive models\n",
        "2. **BERT**: Bidirectional encoder models (good for understanding)\n",
        "3. **T5**: Text-to-text transfer transformer\n",
        "4. **Llama**: Meta's open-source LLM\n",
        "5. **Mistral**: Efficient open-source models\n",
        "\n",
        "Let's explore model information:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get model configuration\n",
        "config = model.config\n",
        "\n",
        "print(\"Model Configuration:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model Type: {config.model_type}\")\n",
        "print(f\"Vocab Size: {config.vocab_size}\")\n",
        "print(f\"Max Position Embeddings: {config.n_positions}\")\n",
        "print(f\"Number of Layers: {config.n_layer}\")\n",
        "print(f\"Hidden Size: {config.n_embd}\")\n",
        "print(f\"Number of Attention Heads: {config.n_head}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Model Size (approx): {total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using LLMs in Agentic AI\n",
        "\n",
        "LLMs serve as the \"reasoning engine\" in agentic AI systems:\n",
        "\n",
        "1. **Understanding**: Process and understand user queries\n",
        "2. **Planning**: Break down complex tasks into steps\n",
        "3. **Decision Making**: Choose which tools/actions to use\n",
        "4. **Response Generation**: Create natural language responses\n",
        "\n",
        "Here's a simple example of how an LLM can be used for agent reasoning:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def agent_reasoning(user_query, available_tools):\n",
        "    \"\"\"\n",
        "    Simulate how an LLM might reason about which tool to use\n",
        "    \"\"\"\n",
        "    # Create a prompt for the LLM\n",
        "    tools_list = \", \".join(available_tools.keys())\n",
        "    prompt = f\"\"\"User query: {user_query}\n",
        "Available tools: {tools_list}\n",
        "\n",
        "Which tool should be used? Respond with just the tool name.\n",
        "Tool:\"\"\"\n",
        "    \n",
        "    # Generate response\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=inputs.shape[1] + 10,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract tool name (simplified)\n",
        "    tool_suggestion = response.split(\"Tool:\")[-1].strip().split()[0] if \"Tool:\" in response else \"unknown\"\n",
        "    \n",
        "    return tool_suggestion, response\n",
        "\n",
        "# Example\n",
        "available_tools = {\n",
        "    \"search\": \"Search the web\",\n",
        "    \"calculate\": \"Perform calculations\",\n",
        "    \"translate\": \"Translate text\"\n",
        "}\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "tool, full_response = agent_reasoning(query, available_tools)\n",
        "\n",
        "print(f\"User Query: {query}\")\n",
        "print(f\"\\nSuggested Tool: {tool}\")\n",
        "print(f\"\\nFull LLM Response:\\n{full_response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n",
        "\n",
        "1. **Model Selection**: Choose models appropriate for your task and resources\n",
        "2. **Token Management**: Be aware of token limits and costs\n",
        "3. **Parameter Tuning**: Experiment with temperature, top-k, top-p\n",
        "4. **Prompt Engineering**: Well-crafted prompts improve results significantly\n",
        "5. **Error Handling**: Always handle potential errors in model inference\n",
        "6. **Resource Management**: Consider using smaller models for development\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "In the next notebook, we'll use LangChain to build more sophisticated AI agents that combine LLMs with tools and memory systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Experiment with different models**: Try loading different models from Hugging Face Hub\n",
        "2. **Parameter tuning**: Experiment with different temperature and top-k values\n",
        "3. **Custom prompt engineering**: Create prompts for specific tasks (summarization, Q&A)\n",
        "4. **Token analysis**: Analyze how different texts are tokenized\n",
        "5. **Build a simple Q&A system**: Use an LLM to answer questions\n",
        "\n",
        "Try these exercises to deepen your understanding of LLMs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
